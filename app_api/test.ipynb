{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-28T14:37:13.009351Z",
     "start_time": "2024-09-28T14:37:12.991813Z"
    }
   },
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.pipelines import MMS_FA as bundle\n",
    "from typing import List\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "from pypinyin import pinyin, lazy_pinyin, Style\n",
    "import whisper\n",
    "import librosa\n",
    "from opencc import OpenCC\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T14:21:33.924083Z",
     "start_time": "2024-09-28T14:21:31.272003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = bundle.get_model()\n",
    "model.to(device)"
   ],
   "id": "e842b34859c549fd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_Wav2Vec2Model(\n",
       "  (model): Wav2Vec2Model(\n",
       "    (feature_extractor): FeatureExtractor(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): ConvLayerBlock(\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "        )\n",
       "        (1-4): 4 x ConvLayerBlock(\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "        )\n",
       "        (5-6): 2 x ConvLayerBlock(\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (encoder): Encoder(\n",
       "      (feature_projection): FeatureProjection(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (transformer): Transformer(\n",
       "        (pos_conv_embed): ConvolutionalPositionalEmbedding(\n",
       "          (conv): ParametrizedConv1d(\n",
       "            1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "            (parametrizations): ModuleDict(\n",
       "              (weight): ParametrizationList(\n",
       "                (0): _WeightNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x EncoderLayer(\n",
       "            (attention): SelfAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): FeedForward(\n",
       "              (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (aux): Linear(in_features=1024, out_features=28, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T14:21:33.954920Z",
     "start_time": "2024-09-28T14:21:33.941234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = bundle.get_tokenizer()\n",
    "aligner = bundle.get_aligner()"
   ],
   "id": "d9b0715b03c89f62",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T14:21:33.985486Z",
     "start_time": "2024-09-28T14:21:33.971467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_alignments(waveform: torch.Tensor, transcript: List[str]):\n",
    "    with torch.inference_mode():\n",
    "        emission, _ = model(waveform.to(device))\n",
    "        token_spans = aligner(emission[0], tokenizer(transcript))\n",
    "    return emission, token_spans"
   ],
   "id": "9c1387d865bf1315",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T14:21:34.016181Z",
     "start_time": "2024-09-28T14:21:34.001671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _score(spans):\n",
    "    return sum(s.score * len(s) for s in spans) / sum(len(s) for s in spans)"
   ],
   "id": "596aba94f4896147",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T14:21:34.047409Z",
     "start_time": "2024-09-28T14:21:34.033897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_alignments(waveform, token_spans, emission, transcript, sample_rate=bundle.sample_rate):\n",
    "    ratio = waveform.size(1) / emission.size(1) / sample_rate\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1)\n",
    "    axes[0].imshow(emission[0].detach().cpu().T, aspect=\"auto\")\n",
    "    axes[0].set_title(\"Emission\")\n",
    "    axes[0].set_xticks([])\n",
    "\n",
    "    axes[1].specgram(waveform[0], Fs=sample_rate)\n",
    "    for t_spans, chars in zip(token_spans, transcript):\n",
    "        t0, t1 = t_spans[0].start, t_spans[-1].end\n",
    "        axes[0].axvspan(t0 - 0.5, t1 - 0.5, facecolor=\"None\", hatch=\"/\", edgecolor=\"white\")\n",
    "        axes[1].axvspan(ratio * t0, ratio * t1, facecolor=\"None\", hatch=\"/\", edgecolor=\"white\")\n",
    "        axes[1].annotate(f\"{_score(t_spans):.2f}\", (ratio * t0, sample_rate * 0.51), annotation_clip=False)\n",
    "\n",
    "        for span, char in zip(t_spans, chars):\n",
    "            t0 = span.start * ratio\n",
    "            axes[1].annotate(char, (t0, sample_rate * 0.55), annotation_clip=False)\n",
    "\n",
    "    axes[1].set_xlabel(\"time [second]\")\n",
    "    fig.tight_layout()"
   ],
   "id": "398424d15d125476",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T14:21:34.078937Z",
     "start_time": "2024-09-28T14:21:34.064404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preview_word(waveform, spans, num_frames, transcript, sample_rate):\n",
    "    ratio = waveform.size(1) / num_frames\n",
    "    x0 = int(ratio * spans[0].start)\n",
    "    x1 = int(ratio * spans[-1].end)\n",
    "    #print(f\"{transcript} ({_score(spans):.2f}): {x0 / sample_rate:.3f} - {x1 / sample_rate:.3f} sec\")\n",
    "    time_StarAndEnd = [ x0 / sample_rate, x1/sample_rate] # 回傳單個字的起始時間與結束時間\n",
    "    segment = waveform[:, x0:x1]\n",
    "    #return IPython.display.Audio(segment.numpy(), rate=sample_rate)\n",
    "    return time_StarAndEnd"
   ],
   "id": "b2a9dce6d5f64",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T14:21:34.717263Z",
     "start_time": "2024-09-28T14:21:34.095484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ],
   "id": "55b466f9df467f34",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |   2416 MiB |  16182 MiB | 134355 MiB | 131939 MiB |\\n|       from large pool |   2413 MiB |  16167 MiB | 131092 MiB | 128679 MiB |\\n|       from small pool |      2 MiB |     14 MiB |   3263 MiB |   3260 MiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |   2416 MiB |  16182 MiB | 134355 MiB | 131939 MiB |\\n|       from large pool |   2413 MiB |  16167 MiB | 131092 MiB | 128679 MiB |\\n|       from small pool |      2 MiB |     14 MiB |   3263 MiB |   3260 MiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |   2414 MiB |  15931 MiB | 133669 MiB | 131254 MiB |\\n|       from large pool |   2412 MiB |  15916 MiB | 130408 MiB | 127995 MiB |\\n|       from small pool |      2 MiB |     14 MiB |   3261 MiB |   3258 MiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |   2568 MiB |  16914 MiB |  16914 MiB |  14346 MiB |\\n|       from large pool |   2564 MiB |  16898 MiB |  16898 MiB |  14334 MiB |\\n|       from small pool |      4 MiB |     16 MiB |     16 MiB |     12 MiB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory | 155322 KiB | 593171 KiB | 150605 MiB | 150454 MiB |\\n|       from large pool | 154112 KiB | 592038 KiB | 147327 MiB | 147177 MiB |\\n|       from small pool |   1210 KiB |   7438 KiB |   3278 MiB |   3277 MiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     847    |    4207    |   34541    |   33694    |\\n|       from large pool |     305    |    1703    |   10817    |   10512    |\\n|       from small pool |     542    |    2504    |   23724    |   23182    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     847    |    4207    |   34541    |   33694    |\\n|       from large pool |     305    |    1703    |   10817    |   10512    |\\n|       from small pool |     542    |    2504    |   23724    |   23182    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |     138    |     759    |     759    |     621    |\\n|       from large pool |     136    |     751    |     751    |     615    |\\n|       from small pool |       2    |       8    |       8    |       6    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      24    |     434    |   15810    |   15786    |\\n|       from large pool |      22    |     429    |    7933    |    7911    |\\n|       from small pool |       2    |      18    |    7877    |    7875    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T14:21:52.519971Z",
     "start_time": "2024-09-28T14:21:34.734736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "raw_data = \".\\\\app_voice\\\\recorded.WAV\"\n",
    "model = whisper.load_model(\"large-v2\")\n",
    "# 使用Whisper進行語音識別\n",
    "result = model.transcribe(raw_data)"
   ],
   "id": "db67137dce07c5ea",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T14:21:52.566524Z",
     "start_time": "2024-09-28T14:21:52.536987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentence =[]\n",
    "cc = OpenCC('s2t')\n",
    "for i in range(len(result['segments'])):\n",
    "    sentence.append([cc.convert(result['segments'][i]['text'].lower().replace('》','').replace('《','').replace('%','').replace('。','').replace('?','').replace('【','').replace('】','').replace('-','').replace('.','').replace(',', '').replace('6','六').replace('4','四').replace('2','二').replace('9','九').replace('8','八').replace('5','五').replace('3','三').replace('0','零').replace('1','一').replace('7','七').replace(' ','').replace('、','')),\n",
    "                     result['segments'][i]['start'],\n",
    "                     result['segments'][i]['end']])\n",
    "    \n",
    "sentence"
   ],
   "id": "e7b2a2359c5420d4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['我要點一杯可樂', 0.0, 3.0]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T14:38:29.611233Z",
     "start_time": "2024-09-28T14:38:29.596504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "audio = AudioSegment.from_wav(raw_data)\n",
    "# print(audio)\n",
    "# print(type(audio))\n",
    "for i in range(len(sentence)):\n",
    "    audio_clip = audio[(sentence[i][1] *1000): (sentence[i][2]*1000)]\n",
    "    audio_clip.export(f\".\\\\sentences\\\\{sentence[i][0].lower()}.wav\", format=\"wav\")"
   ],
   "id": "fa57dba3db594a49",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T14:38:53.131060Z",
     "start_time": "2024-09-28T14:38:53.019285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(0,len(sentence)):#len(sentence)-1\n",
    "    text_normalized = ' '.join(lazy_pinyin(sentence[i][0]))#將文字轉為沒有音調的拼音，lazy_pinyin是陣列所以要再join成字串\n",
    "\n",
    "    waveform, sample_rate = librosa.load(f\".\\\\sentences\\\\{sentence[i][0]}.wav\")\n",
    "    waveform_tensor = torch.tensor(waveform).unsqueeze(0)\n",
    "\n",
    "    transcript = text_normalized.split()\n",
    "    emission, token_spans = compute_alignments(waveform_tensor, transcript)\n",
    "    num_frames = emission.size(1)\n",
    "\n",
    "\n",
    "    #plot_alignments(waveform, token_spans, emission, transcript)\n",
    "\n",
    "    print(\"Raw Transcript: \", sentence[i][0])\n",
    "    print(\"Normalized Transcript: \", text_normalized)\n",
    "    IPython.display.Audio(waveform, rate=sample_rate)\n",
    "\n",
    "    text_raw = sentence[i][0]\n",
    "    word_start_end = []\n",
    "    pinyin_tone = pinyin(text_raw, style=Style.TONE3, heteronym=False)\n",
    "    for j in range(len(transcript)):#len(transcript)\n",
    "        timeStartEnd = preview_word(waveform_tensor, token_spans[j], num_frames, transcript[j], sample_rate)\n",
    "        word_start_end.append([pinyin_tone[j][0], timeStartEnd[0], timeStartEnd[1]])\n",
    "    print(word_start_end)\n",
    "\n",
    "    audio = librosa.load(f\".\\\\sentences\\\\{sentence[i][0]}.wav\")\n",
    "    file_name = sentence[i][0]\n",
    "    for k in range(len(word_start_end)):\n",
    "        segment_audio = audio[word_start_end[k][1] *1000: word_start_end[k][2]*1000]\n",
    "        segment_audio.export(f\".\\\\data\\\\{file_name}-{k}_{word_start_end[k][0]}.wav\", format=\"wav\")\n",
    "    print('------------------------------------------')"
   ],
   "id": "5341311392d737c0",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'tokens'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[28], line 8\u001B[0m\n\u001B[0;32m      5\u001B[0m waveform_tensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(waveform)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m      7\u001B[0m transcript \u001B[38;5;241m=\u001B[39m text_normalized\u001B[38;5;241m.\u001B[39msplit()\n\u001B[1;32m----> 8\u001B[0m emission, token_spans \u001B[38;5;241m=\u001B[39m \u001B[43mcompute_alignments\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwaveform_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtranscript\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m num_frames \u001B[38;5;241m=\u001B[39m emission\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m#plot_alignments(waveform, token_spans, emission, transcript)\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[16], line 3\u001B[0m, in \u001B[0;36mcompute_alignments\u001B[1;34m(waveform, transcript)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_alignments\u001B[39m(waveform: torch\u001B[38;5;241m.\u001B[39mTensor, transcript: List[\u001B[38;5;28mstr\u001B[39m]):\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39minference_mode():\n\u001B[1;32m----> 3\u001B[0m         emission, _ \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwaveform\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m         token_spans \u001B[38;5;241m=\u001B[39m aligner(emission[\u001B[38;5;241m0\u001B[39m], tokenizer(transcript))\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m emission, token_spans\n",
      "File \u001B[1;32mD:\\Coding\\SchoolProject\\testEnv2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Coding\\SchoolProject\\testEnv2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: forward() missing 1 required positional argument: 'tokens'"
     ]
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
