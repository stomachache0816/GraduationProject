{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-28T16:40:33.776082Z",
     "start_time": "2024-09-28T16:40:33.753031Z"
    }
   },
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.pipelines import MMS_FA as bundle\n",
    "from typing import List\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "from pypinyin import pinyin, lazy_pinyin, Style\n",
    "import whisper\n",
    "import librosa\n",
    "from opencc import OpenCC\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:40:36.417145Z",
     "start_time": "2024-09-28T16:40:33.856086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = bundle.get_model()\n",
    "model.to(device)"
   ],
   "id": "e842b34859c549fd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_Wav2Vec2Model(\n",
       "  (model): Wav2Vec2Model(\n",
       "    (feature_extractor): FeatureExtractor(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): ConvLayerBlock(\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "        )\n",
       "        (1-4): 4 x ConvLayerBlock(\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "        )\n",
       "        (5-6): 2 x ConvLayerBlock(\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (encoder): Encoder(\n",
       "      (feature_projection): FeatureProjection(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (transformer): Transformer(\n",
       "        (pos_conv_embed): ConvolutionalPositionalEmbedding(\n",
       "          (conv): ParametrizedConv1d(\n",
       "            1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "            (parametrizations): ModuleDict(\n",
       "              (weight): ParametrizationList(\n",
       "                (0): _WeightNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x EncoderLayer(\n",
       "            (attention): SelfAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): FeedForward(\n",
       "              (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (aux): Linear(in_features=1024, out_features=28, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:40:36.494596Z",
     "start_time": "2024-09-28T16:40:36.480019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = bundle.get_tokenizer()\n",
    "aligner = bundle.get_aligner()"
   ],
   "id": "d9b0715b03c89f62",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:40:36.525314Z",
     "start_time": "2024-09-28T16:40:36.511302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_alignments(waveform: torch.Tensor, transcript: List[str]):\n",
    "    with torch.inference_mode():\n",
    "        emission, _ = model(waveform.to(device))\n",
    "        token_spans = aligner(emission[0], tokenizer(transcript))\n",
    "    return emission, token_spans"
   ],
   "id": "9c1387d865bf1315",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:40:36.554998Z",
     "start_time": "2024-09-28T16:40:36.541090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _score(spans):\n",
    "    return sum(s.score * len(s) for s in spans) / sum(len(s) for s in spans)"
   ],
   "id": "596aba94f4896147",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:40:36.585329Z",
     "start_time": "2024-09-28T16:40:36.571153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_alignments(waveform, token_spans, emission, transcript, sample_rate=bundle.sample_rate):\n",
    "    ratio = waveform.size(1) / emission.size(1) / sample_rate\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1)\n",
    "    axes[0].imshow(emission[0].detach().cpu().T, aspect=\"auto\")\n",
    "    axes[0].set_title(\"Emission\")\n",
    "    axes[0].set_xticks([])\n",
    "\n",
    "    axes[1].specgram(waveform[0], Fs=sample_rate)\n",
    "    for t_spans, chars in zip(token_spans, transcript):\n",
    "        t0, t1 = t_spans[0].start, t_spans[-1].end\n",
    "        axes[0].axvspan(t0 - 0.5, t1 - 0.5, facecolor=\"None\", hatch=\"/\", edgecolor=\"white\")\n",
    "        axes[1].axvspan(ratio * t0, ratio * t1, facecolor=\"None\", hatch=\"/\", edgecolor=\"white\")\n",
    "        axes[1].annotate(f\"{_score(t_spans):.2f}\", (ratio * t0, sample_rate * 0.51), annotation_clip=False)\n",
    "\n",
    "        for span, char in zip(t_spans, chars):\n",
    "            t0 = span.start * ratio\n",
    "            axes[1].annotate(char, (t0, sample_rate * 0.55), annotation_clip=False)\n",
    "\n",
    "    axes[1].set_xlabel(\"time [second]\")\n",
    "    fig.tight_layout()"
   ],
   "id": "398424d15d125476",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:40:36.616654Z",
     "start_time": "2024-09-28T16:40:36.602077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preview_word(waveform, spans, num_frames, transcript, sample_rate):\n",
    "    ratio = waveform.size(1) / num_frames\n",
    "    x0 = int(ratio * spans[0].start)\n",
    "    x1 = int(ratio * spans[-1].end)\n",
    "    #print(f\"{transcript} ({_score(spans):.2f}): {x0 / sample_rate:.3f} - {x1 / sample_rate:.3f} sec\")\n",
    "    time_StarAndEnd = [ x0 / sample_rate, x1/sample_rate] # 回傳單個字的起始時間與結束時間\n",
    "    segment = waveform[:, x0:x1]\n",
    "    #return IPython.display.Audio(segment.numpy(), rate=sample_rate)\n",
    "    return time_StarAndEnd"
   ],
   "id": "b2a9dce6d5f64",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:40:37.470855Z",
     "start_time": "2024-09-28T16:40:36.633320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ],
   "id": "55b466f9df467f34",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |  16862 MiB |  24611 MiB | 420789 MiB | 403927 MiB |\\n|       from large pool |  16845 MiB |  24588 MiB | 408584 MiB | 391739 MiB |\\n|       from small pool |     17 MiB |     23 MiB |  12204 MiB |  12187 MiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |  16862 MiB |  24611 MiB | 420789 MiB | 403927 MiB |\\n|       from large pool |  16845 MiB |  24588 MiB | 408584 MiB | 391739 MiB |\\n|       from small pool |     17 MiB |     23 MiB |  12204 MiB |  12187 MiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |  16598 MiB |  24226 MiB | 417439 MiB | 400840 MiB |\\n|       from large pool |  16581 MiB |  24202 MiB | 405242 MiB | 388660 MiB |\\n|       from small pool |     17 MiB |     23 MiB |  12197 MiB |  12180 MiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |  17390 MiB |  25568 MiB |  42138 MiB |  24748 MiB |\\n|       from large pool |  17370 MiB |  25544 MiB |  42100 MiB |  24730 MiB |\\n|       from small pool |     20 MiB |     24 MiB |     38 MiB |     18 MiB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory | 540494 KiB |    844 MiB | 313817 MiB | 313290 MiB |\\n|       from large pool | 537472 KiB |    843 MiB | 301591 MiB | 301066 MiB |\\n|       from small pool |   3022 KiB |      7 MiB |  12226 MiB |  12223 MiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |    4220    |    6316    |  123958    |  119738    |\\n|       from large pool |    1643    |    2524    |   35035    |   33392    |\\n|       from small pool |    2577    |    3792    |   88923    |   86346    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |    4220    |    6316    |  123958    |  119738    |\\n|       from large pool |    1643    |    2524    |   35035    |   33392    |\\n|       from small pool |    2577    |    3792    |   88923    |   86346    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |     790    |    1150    |    1882    |    1092    |\\n|       from large pool |     780    |    1138    |    1863    |    1083    |\\n|       from small pool |      10    |      12    |      19    |       9    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |     312    |     595    |   56854    |   56542    |\\n|       from large pool |     305    |     585    |   24747    |   24442    |\\n|       from small pool |       7    |      19    |   32107    |   32100    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:41:10.727933Z",
     "start_time": "2024-09-28T16:40:37.487452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "raw_data = \".\\\\app_voice\\\\recorded.WAV\"\n",
    "model = whisper.load_model(\"large-v2\")\n",
    "# 使用Whisper進行語音識別\n",
    "result = model.transcribe(raw_data)"
   ],
   "id": "db67137dce07c5ea",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:41:10.789903Z",
     "start_time": "2024-09-28T16:41:10.744475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentence =[]\n",
    "cc = OpenCC('s2t')\n",
    "for i in range(len(result['segments'])):\n",
    "    sentence.append([cc.convert(result['segments'][i]['text'].lower().replace('》','').replace('《','').replace('%','').replace('。','').replace('?','').replace('【','').replace('】','').replace('-','').replace('.','').replace(',', '').replace('6','六').replace('4','四').replace('2','二').replace('9','九').replace('8','八').replace('5','五').replace('3','三').replace('0','零').replace('1','一').replace('7','七').replace(' ','').replace('、','')),\n",
    "                     result['segments'][i]['start'],\n",
    "                     result['segments'][i]['end']])\n",
    "    \n",
    "sentence"
   ],
   "id": "e7b2a2359c5420d4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['我要喫香蕉', 0.0, 3.0]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:41:10.884574Z",
     "start_time": "2024-09-28T16:41:10.869028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "audio = AudioSegment.from_wav(raw_data)\n",
    "\n",
    "for i in range(len(sentence)):\n",
    "    audio_clip = audio[(sentence[i][1] *1000): (sentence[i][2]*1000)]\n",
    "    audio_clip.export(f\".\\\\sentences\\\\{sentence[i][0].lower()}.wav\", format=\"wav\")\n",
    "print(audio)\n",
    "print(type(audio))"
   ],
   "id": "fa57dba3db594a49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pydub.audio_segment.AudioSegment object at 0x00000286F095C100>\n",
      "<class 'pydub.audio_segment.AudioSegment'>\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:41:11.488862Z",
     "start_time": "2024-09-28T16:41:10.902247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(0,len(sentence)):#len(sentence)-1\n",
    "    text_normalized = ' '.join(lazy_pinyin(sentence[i][0]))#將文字轉為沒有音調的拼音，lazy_pinyin是陣列所以要再join成字串\n",
    "\n",
    "    waveform, sample_rate = librosa.load(f\".\\\\sentences\\\\{sentence[i][0]}.wav\")\n",
    "    waveform_tensor = torch.tensor(waveform).unsqueeze(0)\n",
    "\n",
    "    transcript = text_normalized.split()\n",
    "    emission, token_spans = compute_alignments(waveform_tensor, transcript)\n",
    "    num_frames = emission.size(1)\n",
    "\n",
    "\n",
    "    #plot_alignments(waveform, token_spans, emission, transcript)\n",
    "\n",
    "    print(\"Raw Transcript: \", sentence[i][0])\n",
    "    print(\"Normalized Transcript: \", text_normalized)\n",
    "    IPython.display.Audio(waveform, rate=sample_rate)\n",
    "\n",
    "    text_raw = sentence[i][0]\n",
    "    word_start_end = []\n",
    "    pinyin_tone = pinyin(text_raw, style=Style.TONE3, heteronym=False)\n",
    "    for j in range(len(transcript)):#len(transcript)\n",
    "        timeStartEnd = preview_word(waveform_tensor, token_spans[j], num_frames, transcript[j], sample_rate)\n",
    "        word_start_end.append([pinyin_tone[j][0], timeStartEnd[0], timeStartEnd[1]])\n",
    "    print(word_start_end)\n",
    "\n",
    "    audio = AudioSegment.from_file(f\"sentences\\\\{sentence[i][0]}.wav\")\n",
    "    file_name = sentence[i][0]\n",
    "    for k in range(len(word_start_end)):\n",
    "        segment_audio = audio[word_start_end[k][1] *1000: word_start_end[k][2]*1000]\n",
    "        segment_audio.export(f\"data\\\\{file_name}-{k}_{word_start_end[k][0]}.wav\", format=\"wav\")\n",
    "    print('------------------------------------------')"
   ],
   "id": "5341311392d737c0",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'tokens'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[60], line 8\u001B[0m\n\u001B[0;32m      5\u001B[0m waveform_tensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(waveform)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m      7\u001B[0m transcript \u001B[38;5;241m=\u001B[39m text_normalized\u001B[38;5;241m.\u001B[39msplit()\n\u001B[1;32m----> 8\u001B[0m emission, token_spans \u001B[38;5;241m=\u001B[39m \u001B[43mcompute_alignments\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwaveform_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtranscript\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m num_frames \u001B[38;5;241m=\u001B[39m emission\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m#plot_alignments(waveform, token_spans, emission, transcript)\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[52], line 3\u001B[0m, in \u001B[0;36mcompute_alignments\u001B[1;34m(waveform, transcript)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_alignments\u001B[39m(waveform: torch\u001B[38;5;241m.\u001B[39mTensor, transcript: List[\u001B[38;5;28mstr\u001B[39m]):\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39minference_mode():\n\u001B[1;32m----> 3\u001B[0m         emission, _ \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwaveform\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m         token_spans \u001B[38;5;241m=\u001B[39m aligner(emission[\u001B[38;5;241m0\u001B[39m], tokenizer(transcript))\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m emission, token_spans\n",
      "File \u001B[1;32mD:\\Coding\\SchoolProject\\testEnv2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Coding\\SchoolProject\\testEnv2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: forward() missing 1 required positional argument: 'tokens'"
     ]
    }
   ],
   "execution_count": 60
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
