{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T13:52:37.509733Z",
     "start_time": "2024-10-02T13:52:33.276073Z"
    }
   },
   "source": [
    "import platform\n",
    "print(f\"platform.python_version(): {platform.python_version()}\")\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rc('font', family='Microsoft JhengHei')\n",
    "\n",
    "import requests\n",
    "\n",
    "# 檢查CUDA是否可用\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "platform.python_version(): 3.9.13\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-02T13:52:37.521273Z"
    }
   },
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "#######################################################################\n",
    "#載入mfcc為data\n",
    "npy_file_list = glob(f\"..\\\\data\\\\mfccEduVer\\\\*.npy\")\n",
    "print(f\"type(npy_file_list): {type(npy_file_list)}\")\n",
    "print(f\"len(npy_file_list): {len(npy_file_list)}\")\n",
    "\n",
    "mfcc_list = []\n",
    "\n",
    "for npy_file in tqdm(npy_file_list):\n",
    "    mfcc = np.load(file=npy_file)\n",
    "    mfcc_list.append(mfcc)\n",
    "\n",
    "mfcc_list = np.array(mfcc_list)\n",
    "print(f\"type(mfcc_list): {type(mfcc_list)}\")\n",
    "print(f\"mfcc_list.shape: {mfcc_list.shape}\")\n",
    "\n",
    "all_mfcc = []\n",
    "for mfcc in mfcc_list:\n",
    "    all_mfcc.append(mfcc)\n",
    "\n",
    "data = np.array(all_mfcc)\n",
    "data_len = len(data)\n",
    "#######################################################################\n",
    "\n",
    "#######################################################################\n",
    "#載入labelByself為labels\n",
    "load_data = np.load(f\".\\\\labelByself.npz\")\n",
    "labels = load_data['data']\n",
    "#######################################################################\n",
    "\n",
    "every_epoch = [] \n",
    "every_d_loss = []\n",
    "every_g_loss = []\n",
    "every_DZ = []\n",
    "\n",
    "class PhonemeDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "class ConditionalGenerator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_shape, num_classes):\n",
    "        super(ConditionalGenerator, self).__init__()\n",
    "        self.output_shape = output_shape\n",
    "        self.output_dim = output_shape[0] * output_shape[1]\n",
    "        self.label_embedding = nn.Embedding(num_classes, input_dim)  # 將類別標籤轉換為與 z 相同維度的向量\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim * 2, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.BatchNorm1d(hidden_dim * 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim * 4), \n",
    "            nn.BatchNorm1d(hidden_dim * 4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim * 4, self.output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        label_embedding = self.label_embedding(labels)\n",
    "        input_data = torch.cat([z, label_embedding], dim=1)  # 將噪聲 z 與條件 labels 拼接\n",
    "        x = self.model(input_data)\n",
    "        return x.view(x.size(0), *self.output_shape)\n",
    "\n",
    "\n",
    "class ConditionalDiscriminator(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_dim, num_classes):\n",
    "        super(ConditionalDiscriminator, self).__init__()\n",
    "        self.input_dim = input_shape[0] * input_shape[1]\n",
    "        self.label_embedding = nn.Embedding(num_classes, self.input_dim)  # 將類別標籤轉換為與輸入數據相同維度的向量\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.input_dim * 2, hidden_dim * 8),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim * 8, hidden_dim * 4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim * 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        label_embedding = self.label_embedding(labels)\n",
    "        input_data = torch.cat([x.view(x.size(0), -1), label_embedding], dim=1)  # 將輸入數據與條件 labels 拼接\n",
    "        return self.model(input_data)\n",
    "\n",
    "def get_max_batch_size(model, input_shape):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    max_memory = torch.cuda.get_device_properties(0).total_memory  \n",
    "    reserved_memory = torch.cuda.memory_reserved(0)  \n",
    "    free_memory = max_memory - reserved_memory  \n",
    "\n",
    "    # 計算每個樣本所需的記憶體（假設每個浮點數佔用4字節）\n",
    "    sample_size = (input_shape[0] * input_shape[1] + model.label_embedding.embedding_dim) * 4 + sum(p.numel() for p in model.parameters()) * 4\n",
    "\n",
    "    # 計算最大批量大小\n",
    "    max_batch_size = free_memory // sample_size if sample_size > 0 else 1\n",
    "    return max_batch_size\n",
    "\n",
    "# 超參數\n",
    "input_dim = 572  # 輸入數據的特徵數量13*44=572\n",
    "hidden_dim = 128 # input_dim的一半286\n",
    "generator_learning_rate = 0.00000001\n",
    "discriminator_learning_rate = 0.00000001\n",
    "num_epochs = 5000 # 訓練次數\n",
    "d_steps = 1  # 每個生成器步驟後訓練判別器的步數\n",
    "g_steps = 3  # 每個判別器步驟後訓練生成器的步數\n",
    "\n",
    "# 假設原始MFCC矩陣的形狀為(13, 44)\n",
    "mfcc_shape = (13, 44)\n",
    "num_classes = 1467\n",
    "\n",
    "# 初始化生成器和判別器\n",
    "generator = ConditionalGenerator(input_dim, hidden_dim, mfcc_shape, num_classes).to(device)\n",
    "discriminator = ConditionalDiscriminator(mfcc_shape, hidden_dim, num_classes).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr = generator_learning_rate)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr= discriminator_learning_rate)\n",
    "\n",
    "scheduler_G = optim.lr_scheduler.StepLR(optimizer_G, step_size=500, gamma=0.5) # 每1000步將學習率減半\n",
    "scheduler_D = optim.lr_scheduler.StepLR(optimizer_D, step_size=1000, gamma=0.5) # 每1000步將學習率減半\n",
    "\n",
    "# 計算最大批量大小並創建數據集和數據加載器\n",
    "max_batch_size = get_max_batch_size(discriminator, mfcc_shape)\n",
    "print(f'Maximum batch size that can be used: {max_batch_size}')\n",
    "\n",
    "# 創建數據集和數據加載器\n",
    "dataset = PhonemeDataset(data, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=max_batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, labels) in enumerate(dataloader):\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)  # 標籤同樣移動到設備上\n",
    "        batch_size = data.size(0)\n",
    "\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "        # 訓練判別器\n",
    "        for _ in range(d_steps):\n",
    "            outputs = discriminator(data, labels)  # 輸入數據和對應的標籤\n",
    "            d_loss_real = criterion(outputs, real_labels)\n",
    "            real_score = outputs\n",
    "\n",
    "            z = torch.randn(batch_size, input_dim).to(device)\n",
    "            fake_data = generator(z, labels)  # 使用相同的標籤生成數據\n",
    "            outputs = discriminator(fake_data.detach(), labels)\n",
    "            d_loss_fake = criterion(outputs, fake_labels)\n",
    "            fake_score = outputs\n",
    "\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            optimizer_D.zero_grad()\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        # 訓練生成器\n",
    "        for _ in range(g_steps):\n",
    "            z = torch.randn(batch_size, input_dim).to(device)\n",
    "            fake_data = generator(z, labels)  # 使用相同的標籤生成數據\n",
    "            outputs = discriminator(fake_data, labels)\n",
    "            g_loss = criterion(outputs, real_labels)\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "    # 更新學習率調度器\n",
    "    scheduler_G.step()\n",
    "    scheduler_D.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}, '\n",
    "          f'D(x): {real_score.mean().item():.2f}, D(G(z)): {fake_score.mean().item():.2f}')\n",
    "\n",
    "    every_epoch.append(epoch)\n",
    "    every_d_loss.append(d_loss.item())\n",
    "    every_g_loss.append(g_loss.item())\n",
    "    every_DZ.append(fake_score.mean().item())\n",
    "\n",
    "\n",
    "print('訓練完成！')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(npy_file_list): <class 'list'>\n",
      "len(npy_file_list): 45168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45168/45168 [00:07<00:00, 6253.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(mfcc_list): <class 'numpy.ndarray'>\n",
      "mfcc_list.shape: (45168, 13, 44)\n",
      "Maximum batch size that can be used: 1189\n",
      "Epoch [1/5000], d_loss: 1.4764, g_loss: 0.6701, D(x): 0.49, D(G(z)): 0.51\n",
      "Epoch [2/5000], d_loss: 1.4844, g_loss: 0.6700, D(x): 0.49, D(G(z)): 0.51\n",
      "Epoch [3/5000], d_loss: 1.4965, g_loss: 0.6698, D(x): 0.49, D(G(z)): 0.51\n",
      "Epoch [4/5000], d_loss: 1.4812, g_loss: 0.6696, D(x): 0.49, D(G(z)): 0.51\n",
      "Epoch [5/5000], d_loss: 1.4658, g_loss: 0.6701, D(x): 0.50, D(G(z)): 0.51\n",
      "Epoch [6/5000], d_loss: 1.4839, g_loss: 0.6698, D(x): 0.49, D(G(z)): 0.51\n",
      "Epoch [7/5000], d_loss: 1.4745, g_loss: 0.6701, D(x): 0.49, D(G(z)): 0.51\n",
      "Epoch [8/5000], d_loss: 1.4608, g_loss: 0.6705, D(x): 0.49, D(G(z)): 0.51\n",
      "Epoch [9/5000], d_loss: 1.4752, g_loss: 0.6698, D(x): 0.49, D(G(z)): 0.51\n",
      "Epoch [10/5000], d_loss: 1.4878, g_loss: 0.6697, D(x): 0.49, D(G(z)): 0.51\n",
      "Epoch [11/5000], d_loss: 1.4811, g_loss: 0.6693, D(x): 0.49, D(G(z)): 0.51\n",
      "Epoch [12/5000], d_loss: 1.4662, g_loss: 0.6700, D(x): 0.49, D(G(z)): 0.51\n",
      "Epoch [13/5000], d_loss: 1.4444, g_loss: 0.6699, D(x): 0.50, D(G(z)): 0.51\n",
      "Epoch [14/5000], d_loss: 1.4769, g_loss: 0.6695, D(x): 0.49, D(G(z)): 0.51\n",
      "Epoch [15/5000], d_loss: 1.4624, g_loss: 0.6695, D(x): 0.49, D(G(z)): 0.51\n",
      "Epoch [16/5000], d_loss: 1.4536, g_loss: 0.6700, D(x): 0.50, D(G(z)): 0.51\n",
      "Epoch [17/5000], d_loss: 1.4737, g_loss: 0.6700, D(x): 0.49, D(G(z)): 0.51\n",
      "Epoch [18/5000], d_loss: 1.4543, g_loss: 0.6701, D(x): 0.50, D(G(z)): 0.51\n",
      "Epoch [19/5000], d_loss: 1.4547, g_loss: 0.6695, D(x): 0.50, D(G(z)): 0.51\n",
      "Epoch [20/5000], d_loss: 1.4569, g_loss: 0.6695, D(x): 0.50, D(G(z)): 0.51\n",
      "Epoch [21/5000], d_loss: 1.4506, g_loss: 0.6697, D(x): 0.50, D(G(z)): 0.51\n",
      "Epoch [22/5000], d_loss: 1.4444, g_loss: 0.6695, D(x): 0.50, D(G(z)): 0.51\n",
      "Epoch [23/5000], d_loss: 1.4482, g_loss: 0.6700, D(x): 0.51, D(G(z)): 0.51\n",
      "Epoch [24/5000], d_loss: 1.4512, g_loss: 0.6696, D(x): 0.50, D(G(z)): 0.51\n",
      "Epoch [25/5000], d_loss: 1.4657, g_loss: 0.6698, D(x): 0.50, D(G(z)): 0.51\n",
      "Epoch [26/5000], d_loss: 1.4479, g_loss: 0.6695, D(x): 0.50, D(G(z)): 0.51\n",
      "Epoch [27/5000], d_loss: 1.4445, g_loss: 0.6697, D(x): 0.50, D(G(z)): 0.51\n",
      "Epoch [28/5000], d_loss: 1.4613, g_loss: 0.6695, D(x): 0.50, D(G(z)): 0.51\n",
      "Epoch [29/5000], d_loss: 1.4521, g_loss: 0.6694, D(x): 0.51, D(G(z)): 0.51\n",
      "Epoch [30/5000], d_loss: 1.4439, g_loss: 0.6694, D(x): 0.51, D(G(z)): 0.51\n",
      "Epoch [31/5000], d_loss: 1.4481, g_loss: 0.6697, D(x): 0.50, D(G(z)): 0.51\n",
      "Epoch [32/5000], d_loss: 1.4560, g_loss: 0.6694, D(x): 0.50, D(G(z)): 0.51\n",
      "Epoch [33/5000], d_loss: 1.4277, g_loss: 0.6697, D(x): 0.51, D(G(z)): 0.51\n",
      "Epoch [34/5000], d_loss: 1.4535, g_loss: 0.6695, D(x): 0.50, D(G(z)): 0.51\n",
      "Epoch [35/5000], d_loss: 1.4259, g_loss: 0.6697, D(x): 0.51, D(G(z)): 0.51\n",
      "Epoch [36/5000], d_loss: 1.4337, g_loss: 0.6691, D(x): 0.51, D(G(z)): 0.51\n",
      "Epoch [37/5000], d_loss: 1.4388, g_loss: 0.6693, D(x): 0.51, D(G(z)): 0.51\n",
      "Epoch [38/5000], d_loss: 1.4332, g_loss: 0.6697, D(x): 0.51, D(G(z)): 0.51\n",
      "Epoch [39/5000], d_loss: 1.4376, g_loss: 0.6689, D(x): 0.51, D(G(z)): 0.51\n",
      "Epoch [40/5000], d_loss: 1.4381, g_loss: 0.6689, D(x): 0.51, D(G(z)): 0.51\n",
      "Epoch [41/5000], d_loss: 1.4217, g_loss: 0.6695, D(x): 0.51, D(G(z)): 0.51\n",
      "Epoch [42/5000], d_loss: 1.4270, g_loss: 0.6695, D(x): 0.51, D(G(z)): 0.51\n",
      "Epoch [43/5000], d_loss: 1.4248, g_loss: 0.6691, D(x): 0.51, D(G(z)): 0.51\n",
      "Epoch [44/5000], d_loss: 1.4198, g_loss: 0.6692, D(x): 0.52, D(G(z)): 0.51\n",
      "Epoch [45/5000], d_loss: 1.4153, g_loss: 0.6693, D(x): 0.52, D(G(z)): 0.51\n",
      "Epoch [46/5000], d_loss: 1.4190, g_loss: 0.6687, D(x): 0.52, D(G(z)): 0.51\n",
      "Epoch [47/5000], d_loss: 1.4352, g_loss: 0.6691, D(x): 0.51, D(G(z)): 0.51\n",
      "Epoch [48/5000], d_loss: 1.4297, g_loss: 0.6693, D(x): 0.51, D(G(z)): 0.51\n",
      "Epoch [49/5000], d_loss: 1.4270, g_loss: 0.6688, D(x): 0.51, D(G(z)): 0.51\n",
      "Epoch [50/5000], d_loss: 1.4050, g_loss: 0.6689, D(x): 0.52, D(G(z)): 0.51\n",
      "Epoch [51/5000], d_loss: 1.4182, g_loss: 0.6688, D(x): 0.52, D(G(z)): 0.51\n",
      "Epoch [52/5000], d_loss: 1.4110, g_loss: 0.6686, D(x): 0.52, D(G(z)): 0.51\n",
      "Epoch [53/5000], d_loss: 1.3995, g_loss: 0.6684, D(x): 0.52, D(G(z)): 0.51\n",
      "Epoch [54/5000], d_loss: 1.4258, g_loss: 0.6688, D(x): 0.51, D(G(z)): 0.51\n",
      "Epoch [55/5000], d_loss: 1.4004, g_loss: 0.6690, D(x): 0.52, D(G(z)): 0.51\n",
      "Epoch [56/5000], d_loss: 1.4134, g_loss: 0.6689, D(x): 0.52, D(G(z)): 0.51\n",
      "Epoch [57/5000], d_loss: 1.3996, g_loss: 0.6692, D(x): 0.53, D(G(z)): 0.51\n",
      "Epoch [58/5000], d_loss: 1.3998, g_loss: 0.6689, D(x): 0.52, D(G(z)): 0.51\n",
      "Epoch [59/5000], d_loss: 1.4307, g_loss: 0.6683, D(x): 0.51, D(G(z)): 0.51\n",
      "Epoch [60/5000], d_loss: 1.4138, g_loss: 0.6685, D(x): 0.52, D(G(z)): 0.51\n",
      "Epoch [61/5000], d_loss: 1.4172, g_loss: 0.6692, D(x): 0.52, D(G(z)): 0.51\n",
      "Epoch [62/5000], d_loss: 1.4119, g_loss: 0.6687, D(x): 0.52, D(G(z)): 0.51\n",
      "Epoch [63/5000], d_loss: 1.4066, g_loss: 0.6685, D(x): 0.52, D(G(z)): 0.51\n",
      "Epoch [64/5000], d_loss: 1.3898, g_loss: 0.6685, D(x): 0.53, D(G(z)): 0.51\n",
      "Epoch [65/5000], d_loss: 1.3824, g_loss: 0.6692, D(x): 0.53, D(G(z)): 0.51\n",
      "Epoch [66/5000], d_loss: 1.4037, g_loss: 0.6687, D(x): 0.52, D(G(z)): 0.51\n",
      "Epoch [67/5000], d_loss: 1.3950, g_loss: 0.6690, D(x): 0.53, D(G(z)): 0.51\n",
      "Epoch [68/5000], d_loss: 1.3946, g_loss: 0.6684, D(x): 0.52, D(G(z)): 0.51\n",
      "Epoch [69/5000], d_loss: 1.4031, g_loss: 0.6689, D(x): 0.53, D(G(z)): 0.51\n",
      "Epoch [70/5000], d_loss: 1.3882, g_loss: 0.6687, D(x): 0.53, D(G(z)): 0.51\n",
      "Epoch [71/5000], d_loss: 1.3923, g_loss: 0.6688, D(x): 0.53, D(G(z)): 0.51\n",
      "Epoch [72/5000], d_loss: 1.3850, g_loss: 0.6686, D(x): 0.53, D(G(z)): 0.51\n",
      "Epoch [73/5000], d_loss: 1.3828, g_loss: 0.6688, D(x): 0.53, D(G(z)): 0.51\n",
      "Epoch [74/5000], d_loss: 1.4012, g_loss: 0.6690, D(x): 0.53, D(G(z)): 0.51\n",
      "Epoch [75/5000], d_loss: 1.3936, g_loss: 0.6686, D(x): 0.53, D(G(z)): 0.51\n",
      "Epoch [76/5000], d_loss: 1.3792, g_loss: 0.6688, D(x): 0.54, D(G(z)): 0.51\n",
      "Epoch [77/5000], d_loss: 1.3747, g_loss: 0.6683, D(x): 0.54, D(G(z)): 0.51\n",
      "Epoch [78/5000], d_loss: 1.3951, g_loss: 0.6687, D(x): 0.53, D(G(z)): 0.51\n",
      "Epoch [79/5000], d_loss: 1.3839, g_loss: 0.6692, D(x): 0.53, D(G(z)): 0.51\n",
      "Epoch [80/5000], d_loss: 1.3916, g_loss: 0.6684, D(x): 0.53, D(G(z)): 0.51\n",
      "Epoch [81/5000], d_loss: 1.3609, g_loss: 0.6686, D(x): 0.54, D(G(z)): 0.51\n",
      "Epoch [82/5000], d_loss: 1.3741, g_loss: 0.6685, D(x): 0.54, D(G(z)): 0.51\n",
      "Epoch [83/5000], d_loss: 1.3631, g_loss: 0.6687, D(x): 0.54, D(G(z)): 0.51\n",
      "Epoch [84/5000], d_loss: 1.3721, g_loss: 0.6681, D(x): 0.54, D(G(z)): 0.51\n",
      "Epoch [85/5000], d_loss: 1.3807, g_loss: 0.6679, D(x): 0.54, D(G(z)): 0.51\n",
      "Epoch [86/5000], d_loss: 1.3749, g_loss: 0.6673, D(x): 0.54, D(G(z)): 0.51\n",
      "Epoch [87/5000], d_loss: 1.3624, g_loss: 0.6682, D(x): 0.54, D(G(z)): 0.51\n",
      "Epoch [88/5000], d_loss: 1.3754, g_loss: 0.6684, D(x): 0.54, D(G(z)): 0.51\n",
      "Epoch [89/5000], d_loss: 1.3756, g_loss: 0.6686, D(x): 0.54, D(G(z)): 0.51\n",
      "Epoch [90/5000], d_loss: 1.3676, g_loss: 0.6683, D(x): 0.54, D(G(z)): 0.51\n",
      "Epoch [91/5000], d_loss: 1.3537, g_loss: 0.6685, D(x): 0.55, D(G(z)): 0.51\n",
      "Epoch [92/5000], d_loss: 1.3632, g_loss: 0.6683, D(x): 0.54, D(G(z)): 0.51\n",
      "Epoch [93/5000], d_loss: 1.3624, g_loss: 0.6679, D(x): 0.55, D(G(z)): 0.51\n",
      "Epoch [94/5000], d_loss: 1.3685, g_loss: 0.6685, D(x): 0.54, D(G(z)): 0.51\n",
      "Epoch [95/5000], d_loss: 1.3668, g_loss: 0.6683, D(x): 0.54, D(G(z)): 0.51\n",
      "Epoch [96/5000], d_loss: 1.3508, g_loss: 0.6677, D(x): 0.55, D(G(z)): 0.51\n",
      "Epoch [97/5000], d_loss: 1.3733, g_loss: 0.6682, D(x): 0.54, D(G(z)): 0.51\n",
      "Epoch [98/5000], d_loss: 1.3582, g_loss: 0.6681, D(x): 0.55, D(G(z)): 0.51\n",
      "Epoch [99/5000], d_loss: 1.3595, g_loss: 0.6675, D(x): 0.55, D(G(z)): 0.51\n",
      "Epoch [100/5000], d_loss: 1.3533, g_loss: 0.6682, D(x): 0.55, D(G(z)): 0.51\n",
      "Epoch [101/5000], d_loss: 1.3647, g_loss: 0.6688, D(x): 0.55, D(G(z)): 0.51\n",
      "Epoch [102/5000], d_loss: 1.3554, g_loss: 0.6682, D(x): 0.55, D(G(z)): 0.51\n",
      "Epoch [103/5000], d_loss: 1.3452, g_loss: 0.6682, D(x): 0.55, D(G(z)): 0.51\n",
      "Epoch [104/5000], d_loss: 1.3501, g_loss: 0.6682, D(x): 0.55, D(G(z)): 0.51\n",
      "Epoch [105/5000], d_loss: 1.3524, g_loss: 0.6680, D(x): 0.55, D(G(z)): 0.51\n",
      "Epoch [106/5000], d_loss: 1.3555, g_loss: 0.6682, D(x): 0.55, D(G(z)): 0.51\n",
      "Epoch [107/5000], d_loss: 1.3452, g_loss: 0.6677, D(x): 0.56, D(G(z)): 0.51\n",
      "Epoch [108/5000], d_loss: 1.3449, g_loss: 0.6684, D(x): 0.55, D(G(z)): 0.51\n",
      "Epoch [109/5000], d_loss: 1.3580, g_loss: 0.6679, D(x): 0.55, D(G(z)): 0.51\n",
      "Epoch [110/5000], d_loss: 1.3434, g_loss: 0.6677, D(x): 0.55, D(G(z)): 0.51\n",
      "Epoch [111/5000], d_loss: 1.3552, g_loss: 0.6676, D(x): 0.55, D(G(z)): 0.51\n",
      "Epoch [112/5000], d_loss: 1.3388, g_loss: 0.6674, D(x): 0.56, D(G(z)): 0.51\n",
      "Epoch [113/5000], d_loss: 1.3404, g_loss: 0.6673, D(x): 0.56, D(G(z)): 0.51\n",
      "Epoch [114/5000], d_loss: 1.3306, g_loss: 0.6678, D(x): 0.56, D(G(z)): 0.51\n",
      "Epoch [115/5000], d_loss: 1.3445, g_loss: 0.6675, D(x): 0.56, D(G(z)): 0.51\n",
      "Epoch [116/5000], d_loss: 1.3505, g_loss: 0.6682, D(x): 0.55, D(G(z)): 0.51\n",
      "Epoch [117/5000], d_loss: 1.3250, g_loss: 0.6680, D(x): 0.56, D(G(z)): 0.51\n",
      "Epoch [118/5000], d_loss: 1.3452, g_loss: 0.6679, D(x): 0.56, D(G(z)): 0.51\n",
      "Epoch [119/5000], d_loss: 1.3242, g_loss: 0.6679, D(x): 0.56, D(G(z)): 0.51\n",
      "Epoch [120/5000], d_loss: 1.3274, g_loss: 0.6673, D(x): 0.56, D(G(z)): 0.51\n",
      "Epoch [121/5000], d_loss: 1.3238, g_loss: 0.6673, D(x): 0.56, D(G(z)): 0.51\n",
      "Epoch [122/5000], d_loss: 1.3319, g_loss: 0.6681, D(x): 0.56, D(G(z)): 0.51\n",
      "Epoch [123/5000], d_loss: 1.3296, g_loss: 0.6677, D(x): 0.56, D(G(z)): 0.51\n",
      "Epoch [124/5000], d_loss: 1.3272, g_loss: 0.6675, D(x): 0.56, D(G(z)): 0.51\n",
      "Epoch [125/5000], d_loss: 1.3193, g_loss: 0.6676, D(x): 0.57, D(G(z)): 0.51\n",
      "Epoch [126/5000], d_loss: 1.3118, g_loss: 0.6673, D(x): 0.57, D(G(z)): 0.51\n",
      "Epoch [127/5000], d_loss: 1.3310, g_loss: 0.6671, D(x): 0.56, D(G(z)): 0.51\n",
      "Epoch [128/5000], d_loss: 1.3293, g_loss: 0.6671, D(x): 0.56, D(G(z)): 0.51\n",
      "Epoch [129/5000], d_loss: 1.3282, g_loss: 0.6678, D(x): 0.57, D(G(z)): 0.51\n",
      "Epoch [130/5000], d_loss: 1.3208, g_loss: 0.6670, D(x): 0.57, D(G(z)): 0.51\n",
      "Epoch [131/5000], d_loss: 1.3164, g_loss: 0.6668, D(x): 0.57, D(G(z)): 0.51\n",
      "Epoch [132/5000], d_loss: 1.3108, g_loss: 0.6672, D(x): 0.57, D(G(z)): 0.51\n",
      "Epoch [133/5000], d_loss: 1.3123, g_loss: 0.6675, D(x): 0.57, D(G(z)): 0.51\n",
      "Epoch [134/5000], d_loss: 1.3122, g_loss: 0.6673, D(x): 0.57, D(G(z)): 0.51\n",
      "Epoch [135/5000], d_loss: 1.3249, g_loss: 0.6673, D(x): 0.56, D(G(z)): 0.51\n",
      "Epoch [136/5000], d_loss: 1.3254, g_loss: 0.6671, D(x): 0.56, D(G(z)): 0.51\n",
      "Epoch [137/5000], d_loss: 1.3214, g_loss: 0.6672, D(x): 0.57, D(G(z)): 0.51\n",
      "Epoch [138/5000], d_loss: 1.3012, g_loss: 0.6674, D(x): 0.58, D(G(z)): 0.51\n",
      "Epoch [139/5000], d_loss: 1.3142, g_loss: 0.6673, D(x): 0.57, D(G(z)): 0.51\n",
      "Epoch [140/5000], d_loss: 1.3075, g_loss: 0.6672, D(x): 0.57, D(G(z)): 0.51\n",
      "Epoch [141/5000], d_loss: 1.3089, g_loss: 0.6668, D(x): 0.57, D(G(z)): 0.51\n",
      "Epoch [142/5000], d_loss: 1.3095, g_loss: 0.6670, D(x): 0.57, D(G(z)): 0.51\n",
      "Epoch [143/5000], d_loss: 1.3117, g_loss: 0.6671, D(x): 0.57, D(G(z)): 0.51\n",
      "Epoch [144/5000], d_loss: 1.3117, g_loss: 0.6673, D(x): 0.57, D(G(z)): 0.51\n",
      "Epoch [145/5000], d_loss: 1.3066, g_loss: 0.6671, D(x): 0.57, D(G(z)): 0.51\n",
      "Epoch [146/5000], d_loss: 1.3155, g_loss: 0.6670, D(x): 0.57, D(G(z)): 0.51\n",
      "Epoch [147/5000], d_loss: 1.3061, g_loss: 0.6674, D(x): 0.57, D(G(z)): 0.51\n",
      "Epoch [148/5000], d_loss: 1.2958, g_loss: 0.6677, D(x): 0.58, D(G(z)): 0.51\n",
      "Epoch [149/5000], d_loss: 1.3139, g_loss: 0.6670, D(x): 0.57, D(G(z)): 0.51\n",
      "Epoch [150/5000], d_loss: 1.2841, g_loss: 0.6667, D(x): 0.58, D(G(z)): 0.51\n",
      "Epoch [151/5000], d_loss: 1.2939, g_loss: 0.6667, D(x): 0.58, D(G(z)): 0.51\n",
      "Epoch [152/5000], d_loss: 1.2904, g_loss: 0.6673, D(x): 0.58, D(G(z)): 0.51\n",
      "Epoch [153/5000], d_loss: 1.3048, g_loss: 0.6672, D(x): 0.58, D(G(z)): 0.51\n",
      "Epoch [154/5000], d_loss: 1.2913, g_loss: 0.6666, D(x): 0.58, D(G(z)): 0.51\n",
      "Epoch [155/5000], d_loss: 1.2906, g_loss: 0.6667, D(x): 0.58, D(G(z)): 0.51\n",
      "Epoch [156/5000], d_loss: 1.2855, g_loss: 0.6667, D(x): 0.58, D(G(z)): 0.51\n",
      "Epoch [157/5000], d_loss: 1.3057, g_loss: 0.6667, D(x): 0.57, D(G(z)): 0.51\n",
      "Epoch [158/5000], d_loss: 1.2981, g_loss: 0.6674, D(x): 0.58, D(G(z)): 0.51\n",
      "Epoch [159/5000], d_loss: 1.2887, g_loss: 0.6669, D(x): 0.58, D(G(z)): 0.51\n",
      "Epoch [160/5000], d_loss: 1.2829, g_loss: 0.6671, D(x): 0.59, D(G(z)): 0.51\n",
      "Epoch [161/5000], d_loss: 1.2862, g_loss: 0.6670, D(x): 0.58, D(G(z)): 0.51\n",
      "Epoch [162/5000], d_loss: 1.2905, g_loss: 0.6672, D(x): 0.59, D(G(z)): 0.51\n",
      "Epoch [163/5000], d_loss: 1.2951, g_loss: 0.6664, D(x): 0.58, D(G(z)): 0.51\n",
      "Epoch [164/5000], d_loss: 1.2867, g_loss: 0.6668, D(x): 0.58, D(G(z)): 0.51\n",
      "Epoch [165/5000], d_loss: 1.2761, g_loss: 0.6675, D(x): 0.59, D(G(z)): 0.51\n",
      "Epoch [166/5000], d_loss: 1.2721, g_loss: 0.6663, D(x): 0.59, D(G(z)): 0.51\n",
      "Epoch [167/5000], d_loss: 1.2776, g_loss: 0.6670, D(x): 0.59, D(G(z)): 0.51\n",
      "Epoch [168/5000], d_loss: 1.2759, g_loss: 0.6665, D(x): 0.59, D(G(z)): 0.51\n",
      "Epoch [169/5000], d_loss: 1.2832, g_loss: 0.6665, D(x): 0.59, D(G(z)): 0.51\n",
      "Epoch [170/5000], d_loss: 1.2773, g_loss: 0.6664, D(x): 0.59, D(G(z)): 0.51\n",
      "Epoch [171/5000], d_loss: 1.2750, g_loss: 0.6667, D(x): 0.59, D(G(z)): 0.51\n",
      "Epoch [172/5000], d_loss: 1.2755, g_loss: 0.6670, D(x): 0.59, D(G(z)): 0.51\n",
      "Epoch [173/5000], d_loss: 1.2615, g_loss: 0.6671, D(x): 0.60, D(G(z)): 0.51\n",
      "Epoch [174/5000], d_loss: 1.2712, g_loss: 0.6665, D(x): 0.59, D(G(z)): 0.51\n",
      "Epoch [175/5000], d_loss: 1.2836, g_loss: 0.6662, D(x): 0.59, D(G(z)): 0.51\n",
      "Epoch [176/5000], d_loss: 1.2797, g_loss: 0.6664, D(x): 0.59, D(G(z)): 0.51\n",
      "Epoch [177/5000], d_loss: 1.2708, g_loss: 0.6664, D(x): 0.59, D(G(z)): 0.51\n",
      "Epoch [178/5000], d_loss: 1.2827, g_loss: 0.6663, D(x): 0.59, D(G(z)): 0.51\n",
      "Epoch [179/5000], d_loss: 1.2679, g_loss: 0.6661, D(x): 0.59, D(G(z)): 0.51\n",
      "Epoch [180/5000], d_loss: 1.2629, g_loss: 0.6666, D(x): 0.60, D(G(z)): 0.51\n",
      "Epoch [181/5000], d_loss: 1.2738, g_loss: 0.6664, D(x): 0.59, D(G(z)): 0.51\n",
      "Epoch [182/5000], d_loss: 1.2588, g_loss: 0.6659, D(x): 0.60, D(G(z)): 0.51\n",
      "Epoch [183/5000], d_loss: 1.2659, g_loss: 0.6665, D(x): 0.60, D(G(z)): 0.51\n",
      "Epoch [184/5000], d_loss: 1.2632, g_loss: 0.6666, D(x): 0.60, D(G(z)): 0.51\n",
      "Epoch [185/5000], d_loss: 1.2824, g_loss: 0.6660, D(x): 0.59, D(G(z)): 0.51\n",
      "Epoch [186/5000], d_loss: 1.2621, g_loss: 0.6661, D(x): 0.60, D(G(z)): 0.51\n",
      "Epoch [187/5000], d_loss: 1.2624, g_loss: 0.6660, D(x): 0.60, D(G(z)): 0.51\n",
      "Epoch [188/5000], d_loss: 1.2602, g_loss: 0.6667, D(x): 0.60, D(G(z)): 0.51\n",
      "Epoch [189/5000], d_loss: 1.2580, g_loss: 0.6666, D(x): 0.60, D(G(z)): 0.51\n",
      "Epoch [190/5000], d_loss: 1.2674, g_loss: 0.6661, D(x): 0.60, D(G(z)): 0.51\n",
      "Epoch [191/5000], d_loss: 1.2574, g_loss: 0.6656, D(x): 0.60, D(G(z)): 0.51\n",
      "Epoch [192/5000], d_loss: 1.2693, g_loss: 0.6666, D(x): 0.59, D(G(z)): 0.51\n",
      "Epoch [193/5000], d_loss: 1.2608, g_loss: 0.6658, D(x): 0.60, D(G(z)): 0.51\n",
      "Epoch [194/5000], d_loss: 1.2422, g_loss: 0.6659, D(x): 0.61, D(G(z)): 0.51\n",
      "Epoch [195/5000], d_loss: 1.2573, g_loss: 0.6662, D(x): 0.60, D(G(z)): 0.51\n",
      "Epoch [196/5000], d_loss: 1.2531, g_loss: 0.6661, D(x): 0.60, D(G(z)): 0.51\n",
      "Epoch [197/5000], d_loss: 1.2486, g_loss: 0.6656, D(x): 0.60, D(G(z)): 0.51\n",
      "Epoch [198/5000], d_loss: 1.2454, g_loss: 0.6664, D(x): 0.61, D(G(z)): 0.51\n",
      "Epoch [199/5000], d_loss: 1.2398, g_loss: 0.6657, D(x): 0.61, D(G(z)): 0.51\n",
      "Epoch [200/5000], d_loss: 1.2442, g_loss: 0.6660, D(x): 0.61, D(G(z)): 0.51\n",
      "Epoch [201/5000], d_loss: 1.2431, g_loss: 0.6665, D(x): 0.61, D(G(z)): 0.51\n",
      "Epoch [202/5000], d_loss: 1.2491, g_loss: 0.6662, D(x): 0.61, D(G(z)): 0.51\n",
      "Epoch [203/5000], d_loss: 1.2537, g_loss: 0.6663, D(x): 0.60, D(G(z)): 0.51\n",
      "Epoch [204/5000], d_loss: 1.2323, g_loss: 0.6659, D(x): 0.61, D(G(z)): 0.51\n",
      "Epoch [205/5000], d_loss: 1.2342, g_loss: 0.6658, D(x): 0.61, D(G(z)): 0.51\n",
      "Epoch [206/5000], d_loss: 1.2373, g_loss: 0.6659, D(x): 0.61, D(G(z)): 0.51\n",
      "Epoch [207/5000], d_loss: 1.2460, g_loss: 0.6656, D(x): 0.61, D(G(z)): 0.51\n",
      "Epoch [208/5000], d_loss: 1.2342, g_loss: 0.6657, D(x): 0.61, D(G(z)): 0.51\n",
      "Epoch [209/5000], d_loss: 1.2348, g_loss: 0.6657, D(x): 0.61, D(G(z)): 0.51\n",
      "Epoch [210/5000], d_loss: 1.2338, g_loss: 0.6658, D(x): 0.61, D(G(z)): 0.51\n",
      "Epoch [211/5000], d_loss: 1.2431, g_loss: 0.6662, D(x): 0.61, D(G(z)): 0.51\n",
      "Epoch [212/5000], d_loss: 1.2516, g_loss: 0.6658, D(x): 0.61, D(G(z)): 0.51\n",
      "Epoch [213/5000], d_loss: 1.2285, g_loss: 0.6656, D(x): 0.62, D(G(z)): 0.51\n",
      "Epoch [214/5000], d_loss: 1.2365, g_loss: 0.6656, D(x): 0.62, D(G(z)): 0.51\n",
      "Epoch [215/5000], d_loss: 1.2369, g_loss: 0.6654, D(x): 0.61, D(G(z)): 0.51\n",
      "Epoch [216/5000], d_loss: 1.2495, g_loss: 0.6655, D(x): 0.60, D(G(z)): 0.51\n",
      "Epoch [217/5000], d_loss: 1.2302, g_loss: 0.6660, D(x): 0.61, D(G(z)): 0.51\n",
      "Epoch [218/5000], d_loss: 1.2325, g_loss: 0.6654, D(x): 0.61, D(G(z)): 0.51\n",
      "Epoch [219/5000], d_loss: 1.2433, g_loss: 0.6655, D(x): 0.61, D(G(z)): 0.51\n",
      "Epoch [220/5000], d_loss: 1.2237, g_loss: 0.6658, D(x): 0.62, D(G(z)): 0.51\n",
      "Epoch [221/5000], d_loss: 1.2384, g_loss: 0.6659, D(x): 0.61, D(G(z)): 0.51\n",
      "Epoch [222/5000], d_loss: 1.2339, g_loss: 0.6657, D(x): 0.61, D(G(z)): 0.51\n",
      "Epoch [223/5000], d_loss: 1.2326, g_loss: 0.6653, D(x): 0.61, D(G(z)): 0.51\n",
      "Epoch [224/5000], d_loss: 1.2248, g_loss: 0.6655, D(x): 0.62, D(G(z)): 0.51\n",
      "Epoch [225/5000], d_loss: 1.2165, g_loss: 0.6654, D(x): 0.62, D(G(z)): 0.51\n",
      "Epoch [226/5000], d_loss: 1.2125, g_loss: 0.6656, D(x): 0.63, D(G(z)): 0.51\n",
      "Epoch [227/5000], d_loss: 1.2256, g_loss: 0.6653, D(x): 0.62, D(G(z)): 0.51\n",
      "Epoch [228/5000], d_loss: 1.2380, g_loss: 0.6654, D(x): 0.61, D(G(z)): 0.51\n",
      "Epoch [229/5000], d_loss: 1.2235, g_loss: 0.6648, D(x): 0.62, D(G(z)): 0.51\n",
      "Epoch [230/5000], d_loss: 1.2232, g_loss: 0.6653, D(x): 0.62, D(G(z)): 0.51\n",
      "Epoch [231/5000], d_loss: 1.2153, g_loss: 0.6655, D(x): 0.62, D(G(z)): 0.51\n",
      "Epoch [232/5000], d_loss: 1.2127, g_loss: 0.6665, D(x): 0.63, D(G(z)): 0.51\n",
      "Epoch [233/5000], d_loss: 1.2226, g_loss: 0.6655, D(x): 0.62, D(G(z)): 0.51\n",
      "Epoch [234/5000], d_loss: 1.2072, g_loss: 0.6653, D(x): 0.63, D(G(z)): 0.51\n",
      "Epoch [235/5000], d_loss: 1.2250, g_loss: 0.6649, D(x): 0.62, D(G(z)): 0.51\n",
      "Epoch [236/5000], d_loss: 1.2179, g_loss: 0.6654, D(x): 0.62, D(G(z)): 0.51\n",
      "Epoch [237/5000], d_loss: 1.2110, g_loss: 0.6653, D(x): 0.63, D(G(z)): 0.51\n",
      "Epoch [238/5000], d_loss: 1.2120, g_loss: 0.6653, D(x): 0.63, D(G(z)): 0.51\n",
      "Epoch [239/5000], d_loss: 1.2150, g_loss: 0.6653, D(x): 0.62, D(G(z)): 0.51\n",
      "Epoch [240/5000], d_loss: 1.2052, g_loss: 0.6652, D(x): 0.63, D(G(z)): 0.51\n",
      "Epoch [241/5000], d_loss: 1.2111, g_loss: 0.6652, D(x): 0.63, D(G(z)): 0.51\n",
      "Epoch [242/5000], d_loss: 1.2301, g_loss: 0.6650, D(x): 0.62, D(G(z)): 0.51\n",
      "Epoch [243/5000], d_loss: 1.2121, g_loss: 0.6647, D(x): 0.63, D(G(z)): 0.51\n",
      "Epoch [244/5000], d_loss: 1.1950, g_loss: 0.6645, D(x): 0.64, D(G(z)): 0.51\n",
      "Epoch [245/5000], d_loss: 1.2017, g_loss: 0.6650, D(x): 0.63, D(G(z)): 0.51\n",
      "Epoch [246/5000], d_loss: 1.2139, g_loss: 0.6651, D(x): 0.63, D(G(z)): 0.51\n",
      "Epoch [247/5000], d_loss: 1.2142, g_loss: 0.6649, D(x): 0.63, D(G(z)): 0.51\n",
      "Epoch [248/5000], d_loss: 1.2078, g_loss: 0.6651, D(x): 0.63, D(G(z)): 0.51\n",
      "Epoch [249/5000], d_loss: 1.2082, g_loss: 0.6651, D(x): 0.63, D(G(z)): 0.51\n",
      "Epoch [250/5000], d_loss: 1.1935, g_loss: 0.6647, D(x): 0.64, D(G(z)): 0.51\n",
      "Epoch [251/5000], d_loss: 1.2015, g_loss: 0.6649, D(x): 0.63, D(G(z)): 0.51\n",
      "Epoch [252/5000], d_loss: 1.2073, g_loss: 0.6652, D(x): 0.63, D(G(z)): 0.51\n",
      "Epoch [253/5000], d_loss: 1.2022, g_loss: 0.6652, D(x): 0.63, D(G(z)): 0.51\n",
      "Epoch [254/5000], d_loss: 1.2037, g_loss: 0.6648, D(x): 0.63, D(G(z)): 0.51\n",
      "Epoch [255/5000], d_loss: 1.1918, g_loss: 0.6653, D(x): 0.64, D(G(z)): 0.51\n",
      "Epoch [256/5000], d_loss: 1.1989, g_loss: 0.6649, D(x): 0.63, D(G(z)): 0.51\n",
      "Epoch [257/5000], d_loss: 1.1949, g_loss: 0.6653, D(x): 0.64, D(G(z)): 0.51\n",
      "Epoch [258/5000], d_loss: 1.1809, g_loss: 0.6648, D(x): 0.65, D(G(z)): 0.51\n",
      "Epoch [259/5000], d_loss: 1.1901, g_loss: 0.6646, D(x): 0.64, D(G(z)): 0.51\n",
      "Epoch [260/5000], d_loss: 1.1892, g_loss: 0.6647, D(x): 0.64, D(G(z)): 0.51\n",
      "Epoch [261/5000], d_loss: 1.1866, g_loss: 0.6647, D(x): 0.64, D(G(z)): 0.51\n",
      "Epoch [262/5000], d_loss: 1.1902, g_loss: 0.6652, D(x): 0.64, D(G(z)): 0.51\n",
      "Epoch [263/5000], d_loss: 1.1876, g_loss: 0.6648, D(x): 0.64, D(G(z)): 0.51\n",
      "Epoch [264/5000], d_loss: 1.1960, g_loss: 0.6644, D(x): 0.64, D(G(z)): 0.51\n",
      "Epoch [265/5000], d_loss: 1.1744, g_loss: 0.6642, D(x): 0.65, D(G(z)): 0.51\n",
      "Epoch [266/5000], d_loss: 1.1809, g_loss: 0.6638, D(x): 0.64, D(G(z)): 0.51\n",
      "Epoch [267/5000], d_loss: 1.1826, g_loss: 0.6649, D(x): 0.65, D(G(z)): 0.51\n",
      "Epoch [268/5000], d_loss: 1.1924, g_loss: 0.6642, D(x): 0.64, D(G(z)): 0.51\n",
      "Epoch [269/5000], d_loss: 1.1913, g_loss: 0.6644, D(x): 0.64, D(G(z)): 0.51\n",
      "Epoch [270/5000], d_loss: 1.1843, g_loss: 0.6648, D(x): 0.64, D(G(z)): 0.51\n",
      "Epoch [271/5000], d_loss: 1.1839, g_loss: 0.6648, D(x): 0.64, D(G(z)): 0.51\n",
      "Epoch [272/5000], d_loss: 1.1893, g_loss: 0.6645, D(x): 0.64, D(G(z)): 0.51\n",
      "Epoch [273/5000], d_loss: 1.1806, g_loss: 0.6645, D(x): 0.65, D(G(z)): 0.51\n",
      "Epoch [274/5000], d_loss: 1.1854, g_loss: 0.6645, D(x): 0.64, D(G(z)): 0.51\n",
      "Epoch [275/5000], d_loss: 1.1740, g_loss: 0.6640, D(x): 0.65, D(G(z)): 0.51\n",
      "Epoch [276/5000], d_loss: 1.1865, g_loss: 0.6648, D(x): 0.64, D(G(z)): 0.51\n",
      "Epoch [277/5000], d_loss: 1.1913, g_loss: 0.6646, D(x): 0.64, D(G(z)): 0.51\n",
      "Epoch [278/5000], d_loss: 1.1831, g_loss: 0.6648, D(x): 0.65, D(G(z)): 0.51\n",
      "Epoch [279/5000], d_loss: 1.1740, g_loss: 0.6646, D(x): 0.65, D(G(z)): 0.51\n",
      "Epoch [280/5000], d_loss: 1.1847, g_loss: 0.6643, D(x): 0.64, D(G(z)): 0.51\n",
      "Epoch [281/5000], d_loss: 1.1748, g_loss: 0.6647, D(x): 0.65, D(G(z)): 0.51\n",
      "Epoch [282/5000], d_loss: 1.1719, g_loss: 0.6647, D(x): 0.65, D(G(z)): 0.51\n",
      "Epoch [283/5000], d_loss: 1.1774, g_loss: 0.6641, D(x): 0.65, D(G(z)): 0.51\n",
      "Epoch [284/5000], d_loss: 1.1658, g_loss: 0.6643, D(x): 0.66, D(G(z)): 0.51\n",
      "Epoch [285/5000], d_loss: 1.1726, g_loss: 0.6643, D(x): 0.65, D(G(z)): 0.51\n",
      "Epoch [286/5000], d_loss: 1.1731, g_loss: 0.6644, D(x): 0.65, D(G(z)): 0.51\n",
      "Epoch [287/5000], d_loss: 1.1634, g_loss: 0.6645, D(x): 0.66, D(G(z)): 0.51\n",
      "Epoch [288/5000], d_loss: 1.1709, g_loss: 0.6644, D(x): 0.65, D(G(z)): 0.51\n",
      "Epoch [289/5000], d_loss: 1.1652, g_loss: 0.6646, D(x): 0.66, D(G(z)): 0.51\n",
      "Epoch [290/5000], d_loss: 1.1707, g_loss: 0.6645, D(x): 0.65, D(G(z)): 0.51\n",
      "Epoch [291/5000], d_loss: 1.1580, g_loss: 0.6637, D(x): 0.66, D(G(z)): 0.51\n",
      "Epoch [292/5000], d_loss: 1.1696, g_loss: 0.6643, D(x): 0.66, D(G(z)): 0.51\n",
      "Epoch [293/5000], d_loss: 1.1733, g_loss: 0.6641, D(x): 0.65, D(G(z)): 0.51\n",
      "Epoch [294/5000], d_loss: 1.1586, g_loss: 0.6640, D(x): 0.66, D(G(z)): 0.51\n",
      "Epoch [295/5000], d_loss: 1.1710, g_loss: 0.6643, D(x): 0.65, D(G(z)): 0.51\n",
      "Epoch [296/5000], d_loss: 1.1670, g_loss: 0.6637, D(x): 0.66, D(G(z)): 0.51\n",
      "Epoch [297/5000], d_loss: 1.1627, g_loss: 0.6644, D(x): 0.66, D(G(z)): 0.51\n",
      "Epoch [298/5000], d_loss: 1.1598, g_loss: 0.6639, D(x): 0.66, D(G(z)): 0.51\n",
      "Epoch [299/5000], d_loss: 1.1539, g_loss: 0.6638, D(x): 0.66, D(G(z)): 0.51\n",
      "Epoch [300/5000], d_loss: 1.1671, g_loss: 0.6640, D(x): 0.65, D(G(z)): 0.51\n",
      "Epoch [301/5000], d_loss: 1.1598, g_loss: 0.6636, D(x): 0.66, D(G(z)): 0.51\n",
      "Epoch [302/5000], d_loss: 1.1559, g_loss: 0.6643, D(x): 0.66, D(G(z)): 0.51\n",
      "Epoch [303/5000], d_loss: 1.1645, g_loss: 0.6644, D(x): 0.66, D(G(z)): 0.51\n",
      "Epoch [304/5000], d_loss: 1.1620, g_loss: 0.6639, D(x): 0.66, D(G(z)): 0.51\n",
      "Epoch [305/5000], d_loss: 1.1573, g_loss: 0.6643, D(x): 0.66, D(G(z)): 0.51\n",
      "Epoch [306/5000], d_loss: 1.1740, g_loss: 0.6643, D(x): 0.65, D(G(z)): 0.51\n",
      "Epoch [307/5000], d_loss: 1.1510, g_loss: 0.6632, D(x): 0.66, D(G(z)): 0.51\n",
      "Epoch [308/5000], d_loss: 1.1540, g_loss: 0.6637, D(x): 0.66, D(G(z)): 0.51\n",
      "Epoch [309/5000], d_loss: 1.1481, g_loss: 0.6638, D(x): 0.67, D(G(z)): 0.52\n",
      "Epoch [310/5000], d_loss: 1.1472, g_loss: 0.6640, D(x): 0.67, D(G(z)): 0.51\n",
      "Epoch [311/5000], d_loss: 1.1533, g_loss: 0.6633, D(x): 0.66, D(G(z)): 0.51\n",
      "Epoch [312/5000], d_loss: 1.1548, g_loss: 0.6638, D(x): 0.66, D(G(z)): 0.51\n",
      "Epoch [313/5000], d_loss: 1.1409, g_loss: 0.6646, D(x): 0.67, D(G(z)): 0.51\n",
      "Epoch [314/5000], d_loss: 1.1531, g_loss: 0.6635, D(x): 0.66, D(G(z)): 0.52\n",
      "Epoch [315/5000], d_loss: 1.1533, g_loss: 0.6640, D(x): 0.66, D(G(z)): 0.52\n",
      "Epoch [316/5000], d_loss: 1.1472, g_loss: 0.6639, D(x): 0.67, D(G(z)): 0.51\n",
      "Epoch [317/5000], d_loss: 1.1480, g_loss: 0.6637, D(x): 0.67, D(G(z)): 0.51\n",
      "Epoch [318/5000], d_loss: 1.1504, g_loss: 0.6635, D(x): 0.67, D(G(z)): 0.52\n",
      "Epoch [319/5000], d_loss: 1.1465, g_loss: 0.6633, D(x): 0.67, D(G(z)): 0.52\n",
      "Epoch [320/5000], d_loss: 1.1409, g_loss: 0.6631, D(x): 0.67, D(G(z)): 0.52\n",
      "Epoch [321/5000], d_loss: 1.1377, g_loss: 0.6633, D(x): 0.67, D(G(z)): 0.52\n",
      "Epoch [322/5000], d_loss: 1.1450, g_loss: 0.6639, D(x): 0.67, D(G(z)): 0.51\n",
      "Epoch [323/5000], d_loss: 1.1426, g_loss: 0.6636, D(x): 0.67, D(G(z)): 0.52\n",
      "Epoch [324/5000], d_loss: 1.1439, g_loss: 0.6637, D(x): 0.67, D(G(z)): 0.51\n",
      "Epoch [325/5000], d_loss: 1.1445, g_loss: 0.6637, D(x): 0.67, D(G(z)): 0.52\n",
      "Epoch [326/5000], d_loss: 1.1489, g_loss: 0.6635, D(x): 0.67, D(G(z)): 0.52\n",
      "Epoch [327/5000], d_loss: 1.1474, g_loss: 0.6635, D(x): 0.67, D(G(z)): 0.52\n",
      "Epoch [328/5000], d_loss: 1.1380, g_loss: 0.6635, D(x): 0.67, D(G(z)): 0.51\n",
      "Epoch [329/5000], d_loss: 1.1391, g_loss: 0.6639, D(x): 0.67, D(G(z)): 0.52\n",
      "Epoch [330/5000], d_loss: 1.1377, g_loss: 0.6634, D(x): 0.67, D(G(z)): 0.52\n",
      "Epoch [331/5000], d_loss: 1.1343, g_loss: 0.6637, D(x): 0.68, D(G(z)): 0.52\n",
      "Epoch [332/5000], d_loss: 1.1341, g_loss: 0.6632, D(x): 0.68, D(G(z)): 0.52\n",
      "Epoch [333/5000], d_loss: 1.1266, g_loss: 0.6641, D(x): 0.68, D(G(z)): 0.51\n",
      "Epoch [334/5000], d_loss: 1.1334, g_loss: 0.6633, D(x): 0.68, D(G(z)): 0.52\n",
      "Epoch [335/5000], d_loss: 1.1381, g_loss: 0.6633, D(x): 0.67, D(G(z)): 0.52\n",
      "Epoch [336/5000], d_loss: 1.1362, g_loss: 0.6635, D(x): 0.67, D(G(z)): 0.52\n",
      "Epoch [337/5000], d_loss: 1.1376, g_loss: 0.6633, D(x): 0.67, D(G(z)): 0.52\n",
      "Epoch [338/5000], d_loss: 1.1291, g_loss: 0.6634, D(x): 0.68, D(G(z)): 0.51\n",
      "Epoch [339/5000], d_loss: 1.1232, g_loss: 0.6630, D(x): 0.68, D(G(z)): 0.51\n",
      "Epoch [340/5000], d_loss: 1.1314, g_loss: 0.6632, D(x): 0.68, D(G(z)): 0.52\n",
      "Epoch [341/5000], d_loss: 1.1394, g_loss: 0.6633, D(x): 0.67, D(G(z)): 0.51\n",
      "Epoch [342/5000], d_loss: 1.1207, g_loss: 0.6638, D(x): 0.68, D(G(z)): 0.51\n",
      "Epoch [343/5000], d_loss: 1.1253, g_loss: 0.6633, D(x): 0.68, D(G(z)): 0.52\n",
      "Epoch [344/5000], d_loss: 1.1318, g_loss: 0.6635, D(x): 0.68, D(G(z)): 0.52\n",
      "Epoch [345/5000], d_loss: 1.1309, g_loss: 0.6629, D(x): 0.68, D(G(z)): 0.52\n",
      "Epoch [346/5000], d_loss: 1.1296, g_loss: 0.6635, D(x): 0.68, D(G(z)): 0.52\n",
      "Epoch [347/5000], d_loss: 1.1192, g_loss: 0.6628, D(x): 0.69, D(G(z)): 0.52\n",
      "Epoch [348/5000], d_loss: 1.1291, g_loss: 0.6632, D(x): 0.68, D(G(z)): 0.52\n",
      "Epoch [349/5000], d_loss: 1.1264, g_loss: 0.6629, D(x): 0.68, D(G(z)): 0.52\n",
      "Epoch [350/5000], d_loss: 1.1266, g_loss: 0.6630, D(x): 0.68, D(G(z)): 0.52\n",
      "Epoch [351/5000], d_loss: 1.1317, g_loss: 0.6637, D(x): 0.68, D(G(z)): 0.52\n",
      "Epoch [352/5000], d_loss: 1.1170, g_loss: 0.6634, D(x): 0.69, D(G(z)): 0.51\n",
      "Epoch [353/5000], d_loss: 1.1104, g_loss: 0.6628, D(x): 0.69, D(G(z)): 0.52\n",
      "Epoch [354/5000], d_loss: 1.1199, g_loss: 0.6632, D(x): 0.68, D(G(z)): 0.52\n",
      "Epoch [355/5000], d_loss: 1.1187, g_loss: 0.6631, D(x): 0.69, D(G(z)): 0.52\n",
      "Epoch [356/5000], d_loss: 1.1125, g_loss: 0.6631, D(x): 0.69, D(G(z)): 0.52\n",
      "Epoch [357/5000], d_loss: 1.1134, g_loss: 0.6628, D(x): 0.69, D(G(z)): 0.52\n",
      "Epoch [358/5000], d_loss: 1.1225, g_loss: 0.6635, D(x): 0.68, D(G(z)): 0.52\n",
      "Epoch [359/5000], d_loss: 1.1205, g_loss: 0.6630, D(x): 0.68, D(G(z)): 0.52\n",
      "Epoch [360/5000], d_loss: 1.1122, g_loss: 0.6628, D(x): 0.69, D(G(z)): 0.52\n",
      "Epoch [361/5000], d_loss: 1.1050, g_loss: 0.6629, D(x): 0.70, D(G(z)): 0.52\n",
      "Epoch [362/5000], d_loss: 1.1095, g_loss: 0.6628, D(x): 0.69, D(G(z)): 0.52\n",
      "Epoch [363/5000], d_loss: 1.1196, g_loss: 0.6632, D(x): 0.68, D(G(z)): 0.52\n",
      "Epoch [364/5000], d_loss: 1.1095, g_loss: 0.6634, D(x): 0.69, D(G(z)): 0.52\n",
      "Epoch [365/5000], d_loss: 1.1111, g_loss: 0.6634, D(x): 0.69, D(G(z)): 0.52\n",
      "Epoch [366/5000], d_loss: 1.1036, g_loss: 0.6630, D(x): 0.70, D(G(z)): 0.52\n",
      "Epoch [367/5000], d_loss: 1.1052, g_loss: 0.6633, D(x): 0.69, D(G(z)): 0.52\n",
      "Epoch [368/5000], d_loss: 1.1104, g_loss: 0.6628, D(x): 0.69, D(G(z)): 0.52\n",
      "Epoch [369/5000], d_loss: 1.1196, g_loss: 0.6628, D(x): 0.68, D(G(z)): 0.52\n",
      "Epoch [370/5000], d_loss: 1.1040, g_loss: 0.6627, D(x): 0.70, D(G(z)): 0.52\n",
      "Epoch [371/5000], d_loss: 1.1040, g_loss: 0.6637, D(x): 0.70, D(G(z)): 0.52\n",
      "Epoch [372/5000], d_loss: 1.1049, g_loss: 0.6622, D(x): 0.69, D(G(z)): 0.52\n",
      "Epoch [373/5000], d_loss: 1.1135, g_loss: 0.6633, D(x): 0.69, D(G(z)): 0.52\n",
      "Epoch [374/5000], d_loss: 1.1078, g_loss: 0.6629, D(x): 0.69, D(G(z)): 0.52\n",
      "Epoch [375/5000], d_loss: 1.1017, g_loss: 0.6621, D(x): 0.70, D(G(z)): 0.52\n",
      "Epoch [376/5000], d_loss: 1.1062, g_loss: 0.6627, D(x): 0.69, D(G(z)): 0.52\n",
      "Epoch [377/5000], d_loss: 1.0956, g_loss: 0.6627, D(x): 0.70, D(G(z)): 0.52\n",
      "Epoch [378/5000], d_loss: 1.0997, g_loss: 0.6622, D(x): 0.70, D(G(z)): 0.52\n",
      "Epoch [379/5000], d_loss: 1.0963, g_loss: 0.6625, D(x): 0.70, D(G(z)): 0.52\n",
      "Epoch [380/5000], d_loss: 1.1009, g_loss: 0.6625, D(x): 0.70, D(G(z)): 0.52\n",
      "Epoch [381/5000], d_loss: 1.0928, g_loss: 0.6629, D(x): 0.70, D(G(z)): 0.52\n",
      "Epoch [382/5000], d_loss: 1.0929, g_loss: 0.6620, D(x): 0.70, D(G(z)): 0.52\n",
      "Epoch [383/5000], d_loss: 1.0975, g_loss: 0.6622, D(x): 0.70, D(G(z)): 0.52\n",
      "Epoch [384/5000], d_loss: 1.0981, g_loss: 0.6630, D(x): 0.70, D(G(z)): 0.52\n",
      "Epoch [385/5000], d_loss: 1.1037, g_loss: 0.6626, D(x): 0.70, D(G(z)): 0.52\n",
      "Epoch [386/5000], d_loss: 1.0920, g_loss: 0.6625, D(x): 0.70, D(G(z)): 0.52\n",
      "Epoch [387/5000], d_loss: 1.1011, g_loss: 0.6623, D(x): 0.70, D(G(z)): 0.52\n",
      "Epoch [388/5000], d_loss: 1.0967, g_loss: 0.6623, D(x): 0.70, D(G(z)): 0.52\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 171\u001B[0m\n\u001B[0;32m    169\u001B[0m     d_loss \u001B[38;5;241m=\u001B[39m d_loss_real \u001B[38;5;241m+\u001B[39m d_loss_fake\n\u001B[0;32m    170\u001B[0m     optimizer_D\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m--> 171\u001B[0m     \u001B[43md_loss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    172\u001B[0m     optimizer_D\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m    174\u001B[0m \u001B[38;5;66;03m# 訓練生成器\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Coding\\SchoolProject\\testEnv2\\lib\\site-packages\\torch\\_tensor.py:521\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    512\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    513\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    514\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    519\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    520\u001B[0m     )\n\u001B[1;32m--> 521\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    522\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    523\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Coding\\SchoolProject\\testEnv2\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    284\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    286\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    288\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 289\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    290\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    291\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    292\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    293\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    294\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    295\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    296\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    297\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Coding\\SchoolProject\\testEnv2\\lib\\site-packages\\torch\\autograd\\graph.py:769\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    767\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    768\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 769\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    770\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    771\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    772\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    773\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# DISCORD -> 設定 -> 整合 -> Webhook -> 新 Webhook > -> 複製 Webhook 網址\n",
    "DISCORD_WEBHOOK_URL = \"https://discord.com/api/webhooks/1287413088970346557/30gx7NdIfSxS1BRWk28IRkOHJeoET-ihIN_KAjYeXYkrpPeI0hBnE-68AHzhpTR4h3et\"\n",
    "requests.post(\n",
    "    url=DISCORD_WEBHOOK_URL,\n",
    "    data={\"content\": \"cGan訓練已完成!\"}\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "torch.save(generator.state_dict(), f\".\\\\cGAN_generator_edu_model.pth\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.plot(every_epoch,  every_d_loss, linestyle='-', label='Discriminator Loss')\n",
    "plt.plot(every_epoch,  every_g_loss, linestyle='-', label='Generator Loss')\n",
    "plt.title(f'在{data_len}筆資料和{num_epochs}次epoch下d_loss與g_loss的變化')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('d_loss')\n",
    "\n",
    "\n",
    "plt.legend() \n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class ConditionalGenerator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_shape, num_classes):\n",
    "        super(ConditionalGenerator, self).__init__()\n",
    "        self.output_shape = output_shape\n",
    "        self.output_dim = output_shape[0] * output_shape[1]\n",
    "        self.label_embedding = nn.Embedding(num_classes, input_dim)  # 將類別標籤轉換為與 z 相同維度的向量\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim * 2, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.BatchNorm1d(hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim * 4), \n",
    "            nn.BatchNorm1d(hidden_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 4, self.output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        label_embedding = self.label_embedding(labels)\n",
    "        input_data = torch.cat([z, label_embedding], dim=1)  # 將噪聲 z 與條件 labels 拼接\n",
    "        x = self.model(input_data)\n",
    "        return x.view(x.size(0), *self.output_shape)\n",
    "    \n",
    "# 參數\n",
    "input_dim =  2048 \n",
    "hidden_dim = 128\n",
    "mfcc_shape = (13, 44)\n",
    "num_classes = 1467\n",
    "\n",
    "# 初始化生成器\n",
    "generator = ConditionalGenerator(input_dim, hidden_dim, mfcc_shape, num_classes).to(device)\n",
    "# 載入儲存的生成器權重\n",
    "# generator.load_state_dict(torch.load(f\".\\\\cGAN_generator_model.pth\"))\n",
    "generator.load_state_dict(torch.load(f\".\\\\cGAN_generator_edu_model.pth\", weights_only=True), strict=False)\n",
    "# 確保模型設定為評估模式\n",
    "generator.eval()\n",
    "\n",
    "batch_size = 100\n",
    "z = torch.randn(batch_size, input_dim).to(device)\n",
    "\n",
    "labels = torch.tensor([10] * batch_size).to(device)  # 生成標籤為 10 的資料\n",
    "fake_data = generator(z, labels)\n",
    "\n",
    "print('確認單筆資料為',len(fake_data[0]),'x',len(fake_data[0][0]),'與預計輸出為 13x14')\n",
    "print('一次生成',len(fake_data),'個 fake data')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fake_data[0]"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
